
# Hacker News Crawler

A functionalâ€‘first Python web crawler that scrapes the topâ€¯30 posts from [Hacker News](https://news.ycombinator.com), lets you filter them by title length, and logs every run for basic usage analytics using a file-based SQLite database.

---

## Project Structure

```

hackernews\_crawler/
â”œâ”€â”€ hn\_crawler/
â”‚   â”œâ”€â”€ crawler.py
â”‚   â”œâ”€â”€ filterer.py
â”‚   â”œâ”€â”€ logger.py          # logs to SQLite
â”‚   â”œâ”€â”€ models.py          # SQLAlchemy model
â”‚   â”œâ”€â”€ db.py              # engine/session for SQLite
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ usage.db               # autoâ€‘created SQLite file (excluded via .gitignore)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ env.tpl                # environment variable template
â”œâ”€â”€ .env                   # symlink or copy of env.tpl (git-ignored)
â””â”€â”€ README.md

````

---

## Features

| Feature          | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| **Scraping**     | Pulls rank, title, points, and comment count for the firstâ€¯30 HN items.     |
| **Two filters**  | â€¢ `filter1` â€“ titles **more than 5 words**, ordered by **comments** (desc)  <br> â€¢ `filter2` â€“ titles **â‰¤ 5 words**, ordered by **points** (desc) |
| **Usage logging**| Logs timestamp, filter type, runtime, and result count to an SQLite file.   |
| **Functional core** | Pure, testable functions with wrapper classes for clarity.               |

---

## Requirements

- **Python 3.9+**
- **[Pipenv](https://pipenv.pypa.io/en/latest/)** (recommended)

> ðŸ’¡ Prefer `pip` + venv? Install from `requirements.txt` and activate the virtual environment normally.

---

## Installation & Setup

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/hackernews-crawler.git
cd hackernews-crawler
````

### 2. Install Pipenv (if needed)

```bash
pip install --user pipenv
```

### 3. Create your local environment file

```bash
cp env.tpl env.dev
ln -s env.dev .env       # Optional but recommended
```

This sets the SQLite database path and other environment variables. If `.env` is missing, the crawler will still default to `./usage.db`.

### 4. Install dependencies and enter the environment

```bash
pipenv install
pipenv shell
```

> Use `exit` or `deactivate` to leave the environment.

---

##  Running the Crawler

```bash
# Filter 1 â€“ titles with >5 words, ordered by comments
python -m hn_crawler.main filter1

# Filter 2 â€“ titles with â‰¤5 words, ordered by points
python -m hn_crawler.main filter2
```

âœ… Each execution logs metadata to `usage.db`, automatically creating the file and tables if they donâ€™t exist.

---

## ðŸ§ª Running Tests (if present)

If youâ€™ve written tests with `pytest`:

```bash
pytest
```

---

## Roadmap / Future Improvements

| Feature           | Status     | Notes                                                |
| ----------------- | ---------- | ---------------------------------------------------- |
| Dockerize app     | ðŸš§ Planned | Add Dockerfile & Compose support                     |
| REST API          | ðŸš§ Planned | Use FastAPI to expose a `/filter` endpoint           |
| Job scheduler     | ðŸš§ Planned | Run crawler periodically via APScheduler             |
| CLI upgrade       | ðŸš§ Planned | Improve UX with `click` or `typer`                   |
| Analytics command | ðŸš§ Planned | CLI to query `usage.db` stats                        |
| CI/CD pipeline    | ðŸš§ Planned | GitHub Actions for linting, testing, and image build |

---

## Contributing

1. Fork the repo & create a branch:
   `git checkout -b feature/my-feature`
2. Commit your changes:
   `git commit -m 'Add feature X'`
3. Push to GitHub:
   `git push origin feature/my-feature`
4. Open a pull request

---

## License

MIT Â© 2025 Elijah Hunt

